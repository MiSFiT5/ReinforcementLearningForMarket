{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共享层和专用层分别处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 假设已有的 name_count, A, B 数据\n",
    "name_count = {\n",
    "    'var1': 1, 'var2': 2, 'var3': 3,  # 示例数据，实际应包含所有73个配置\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# 定义 A 和 B\n",
    "A = np.random.uniform(-1, 1, size=(3, 73))  # 假设 A 的真实数据\n",
    "B = np.random.uniform(-1, 1, size=(3,))    # 假设 B 的真实数据\n",
    "\n",
    "# 解析配置变量\n",
    "variables = list(name_count.keys())\n",
    "participation_count = list(name_count.values())\n",
    "\n",
    "# 找出共享变量的索引\n",
    "shared_indices = [i for i, count in enumerate(participation_count) if count > 1]\n",
    "shared_size = len(shared_indices)\n",
    "\n",
    "# 自定义环境类，包含 KPI 计算逻辑\n",
    "class FactoryEnv(gym.Env):\n",
    "    def __init__(self, state_size, action_size, A, B):\n",
    "        super(FactoryEnv, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(state_size,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(action_size,), dtype=np.float32)\n",
    "        self.state = np.random.uniform(-1, 1, size=(state_size,))\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state = np.clip(self.state + action, -1, 1)\n",
    "        kpi1 = np.dot(self.A[0], self.state) + self.B[0]\n",
    "        kpi2 = np.dot(self.A[1], self.state) + self.B[1]\n",
    "        kpi3 = np.dot(self.A[2], self.state) + self.B[2]\n",
    "        reward_vector = np.array([kpi1, kpi2, kpi3])  # 多目标奖励\n",
    "        done = False  # 根据需要设置终止条件\n",
    "        return self.state, reward_vector, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-1, 1, size=(self.state_size,))\n",
    "        return self.state\n",
    "\n",
    "# 定义 Actor 和 Critic 网络\n",
    "class SharedActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        super(SharedActor, self).__init__()\n",
    "        self.shared_fc = nn.Linear(shared_size, 128)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, 128)\n",
    "        self.fc1 = nn.Linear(256 + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class SharedCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        super(SharedCritic, self).__init__()\n",
    "        self.shared_fc = nn.Linear(shared_size, 128)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, 128)\n",
    "        self.fc1 = nn.Linear(256 + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = torch.relu(self.shared_fc(state[:, shared_indices]))\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.actor = SharedActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.critic = SharedCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_actor = SharedActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_critic = SharedCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, A=A, B=B)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3)\n",
    "\n",
    "# 开始训练\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    while not done:\n",
    "        action = agent.act(state, weights)\n",
    "        next_state, reward_vector, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        episode_reward += np.dot(weights, reward_vector)\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 自定义多头注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)  # 增加批次维度\n",
    "        attn_output, _ = self.attention(x, x, x)  # 自注意力\n",
    "        return attn_output.squeeze(0)  # 移除批次维度\n",
    "\n",
    "# Actor 网络，带多头注意力机制\n",
    "class AttentionActor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=128, num_heads=4):\n",
    "        super(AttentionActor, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state, weights):\n",
    "        shared_features = state[:, shared_indices]\n",
    "        shared_features = self.shared_attention(shared_features)\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "# Critic 网络，带多头注意力机制\n",
    "class AttentionCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size, embed_dim=128, num_heads=4):\n",
    "        super(AttentionCritic, self).__init__()\n",
    "        self.shared_attention = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.individual_fc = nn.Linear(state_size - shared_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(2 * embed_dim + action_size + weight_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state, action, weights):\n",
    "        shared_features = state[:, shared_indices]\n",
    "        shared_features = self.shared_attention(shared_features)\n",
    "        individual_features = torch.relu(self.individual_fc(state[:, [i for i in range(state.size(1)) if i not in shared_indices]]))\n",
    "        combined = torch.cat([shared_features, individual_features, action, weights], dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# 定义 MORL 智能体\n",
    "class MORLAgent:\n",
    "    def __init__(self, state_size, action_size, shared_size, weight_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.weight_size = weight_size\n",
    "        self.actor = AttentionActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.critic = AttentionCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_actor = AttentionActor(state_size, action_size, shared_size, weight_size)\n",
    "        self.target_critic = AttentionCritic(state_size, action_size, shared_size, weight_size)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # 初始化目标网络参数\n",
    "        self._update_target(self.target_actor, self.actor, 1.0)\n",
    "        self._update_target(self.target_critic, self.critic, 1.0)\n",
    "\n",
    "    def _update_target(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def act(self, state, weights):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        weights = torch.FloatTensor(weights).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state, weights).squeeze(0).numpy()\n",
    "        return np.clip(action + np.random.normal(0, 0.1, size=self.action_size), -1, 1)\n",
    "\n",
    "    def remember(self, state, action, reward_vector, next_state, done, weights):\n",
    "        weighted_reward = np.dot(weights, reward_vector)\n",
    "        self.memory.append((state, action, weighted_reward, next_state, done, weights))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, weights = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1)\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        # Critic 更新\n",
    "        next_actions = self.target_actor(next_states, weights)\n",
    "        next_q_values = self.target_critic(next_states, next_actions, weights)\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        q_values = self.critic(states, actions, weights)\n",
    "        critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor 更新\n",
    "        actions_pred = self.actor(states, weights)\n",
    "        actor_loss = -self.critic(states, actions_pred, weights).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # 软更新目标网络\n",
    "        self._update_target(self.target_actor, self.actor, self.tau)\n",
    "        self._update_target(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "# 初始化环境和智能体\n",
    "env = FactoryEnv(state_size=73, action_size=73, A=A, B=B)\n",
    "agent = MORLAgent(state_size=73, action_size=73, shared_size=len(shared_indices), weight_size=3)\n",
    "\n",
    "# 开始训练\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    weights = np.random.dirichlet(np.ones(3), size=1)[0]\n",
    "    while not done:\n",
    "        action = agent.act(state, weights)\n",
    "        next_state, reward_vector, done, _ = env.step(action)\n",
    "        agent.remember(state, action, reward_vector, next_state, done, weights)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        episode_reward += np.dot(weights, reward_vector)\n",
    "    print(f\"Episode {episode + 1}, Weighted Reward: {episode_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
